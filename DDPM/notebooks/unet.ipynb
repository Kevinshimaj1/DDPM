{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03bcbf79-528e-43eb-9af8-1c010fd24aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1ff2e7e-e934-4e03-8e8d-a85135c92a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carica configurazione\n",
    "with open('../configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "T = config['diffusion']['T']\n",
    "beta_start = config['diffusion']['beta_start']\n",
    "beta_end = config['diffusion']['beta_end']\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faea2619-36d8-4fa5-af3e-c57348b2a338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def get_sinusoidal_embedding(self, t):\n",
    "        t = t.float()\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.arange(half_dim, dtype=torch.float32)\n",
    "        freqs = 10000 ** (-freqs / half_dim)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, t):\n",
    "        emb = self.get_sinusoidal_embedding(t)\n",
    "        emb = self.embedding(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf497b43-e7dc-40e8-bd3d-bae128583fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels): # canali in input = 1 (immmagini MNIST) e out_channels ad es = 64 per il primo livello\n",
    "        super().__init__() # chiamo il costruttore della classe genitore nn.Module\n",
    "        self.conv = nn.Sequential( # inizio a definire la sequenza di layer\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, out_channels), # divido i canali in 8 gruppi, migliorando la stabilità del training\n",
    "            nn.SiLU(), # applica la funzione di attivazione SiLu\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), # definisco la seconda convoluzione 2d del blocco\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "    # logica del passaggio dell'input attraverso il blocco\n",
    "    def forward(self, x): # x è un tensore di forma [batch_size, in_channels, height, width]\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6f8ce08-70de-404e-a0e5-9d83677235ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNet sarà la rete neurale principale del DDPM, responsabile di prevedere il rumore \n",
    "class UNet(nn.Module):\n",
    "    # in_channels e out_channels hanno stesso valore perchè l'output da prevedere ha la stessa forma dell'input\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=64, time_dim=128): # base_channels è il numero di canali nel primo blocco (tale valore aumenta in downsampling)\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim # dimensione dell'embedding\n",
    "        self.time_emb = TimeEmbedding(time_dim) # trasformo il time-step t in un embedding\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels) # primo blocco convoluzionale dell'encoder\n",
    "        self.pool1 = nn.MaxPool2d(2) # da 28x28 riduco la risoluzione spaziale a 14x14\n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2) # 128, 14x14\n",
    "        self.pool2 = nn.MaxPool2d(2) # da 14x14 riduco la risoluzione a 7x7\n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)\n",
    "        self.bottleneck = ConvBlock(base_channels * 4, base_channels * 4)\n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2) # aumento la risoluzione a 14x14 e dimezzo il numero di canali, primo layer di upsampling\n",
    "        self.dec1 = ConvBlock(base_channels * 4, base_channels * 2) # primo blocco convoluzionale del decoder\n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2) # aumento la risoluzione e dimezzo il numero di canali\n",
    "        self.dec2 = ConvBlock(base_channels * 2, base_channels)\n",
    "        self.out = nn.Conv2d(base_channels, out_channels, kernel_size=3, padding=1) # layer finale per produrre l'output\n",
    "        self.time_proj1 = nn.Linear(time_dim, base_channels) # embedding da proiettare per il livello 1\n",
    "        self.time_proj2 = nn.Linear(time_dim, base_channels * 2) # embedding da proiettare per il livello 2\n",
    "        self.time_proj3 = nn.Linear(time_dim, base_channels * 4) # embedding da proiettare per il livello 3\n",
    "        \n",
    "    def forward(self, x, t): # x = immagine rumorosa, t = time-step\n",
    "        t_emb = self.time_emb(t)\n",
    "        e1 = self.enc1(x) + self.time_proj1(t_emb)[:, :, None, None] # applico il primo blocco convoluzionale dell'encoder (feature + embedding temporale)\n",
    "        d1 = self.pool1(e1) # applico il primo max pooling\n",
    "        e2 = self.enc2(d1) + self.time_proj2(t_emb)[:, :, None, None]\n",
    "        d2 = self.pool2(e2) # secondo max pooling\n",
    "        e3 = self.enc3(d2) + self.time_proj3(t_emb)[:, :, None, None] # terzo blocco convoluzionale\n",
    "        b = self.bottleneck(e3)\n",
    "        u1 = self.up1(b) # primo upsampling\n",
    "        u1 = torch.cat([u1, e2], dim=1) # concateno con le feature dell'encoder, skip connection\n",
    "        d1 = self.dec1(u1) # primo blocco del decoder\n",
    "        u2 = self.up2(d1)\n",
    "        u2 = torch.cat([u2,e1], dim = 1) # concateno con e1 [64,28,28] cat [64,28,28] -> [128,28,28]\n",
    "        d2 = self.dec2(u2) # secondo blocco del decoder\n",
    "        out = self.out(d2) # layer finale\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd83859b-ffa6-4804-9a0b-cfcc88fba9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma dell'output: torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "unet = UNet()  # Inizializzo la U-Net\n",
    "x = torch.randn(4, 1, 28, 28)  # Batch di 4 immagini MNIST\n",
    "t = torch.tensor([0, 100, 500, 999])  # Timestep\n",
    "out = unet(x, t)  # Passo input e timestep alla U-Net\n",
    "print(f\"Forma dell'output: {out.shape}\")  # Dovrebbe essere [4, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57115978-d4cd-4a15-9bc6-19e6b9cd9381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
