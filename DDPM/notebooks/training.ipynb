{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d00f52-6964-4914-b3e2-070e9eae20f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('../configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "T = config['diffusion']['T']\n",
    "beta_start = config['diffusion']['beta_start']\n",
    "beta_end = config['diffusion']['beta_end']\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "\n",
    "epochs = 30\n",
    "checkpoint_dir = '../checkpoints' # salvo i pesi del modello in questa directory\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5548b07-534f-4e5b-a42d-b7dc1ea2cb81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def get_sinusoidal_embedding(self, t):\n",
    "        t = t.to(dtype=torch.float32) \n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.arange(half_dim, dtype=torch.float32, device=t.device)\n",
    "        freqs = 10000 ** (-freqs / half_dim)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, t):\n",
    "        emb = self.get_sinusoidal_embedding(t)\n",
    "        emb = self.embedding(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e3cefee-e270-4a8d-8e00-824ed0cfe865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6410cc0-544a-4e61-9b9c-290ee6a383cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.time_emb = TimeEmbedding(time_dim)\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)\n",
    "        self.bottleneck = ConvBlock(base_channels * 4, base_channels * 4)\n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_channels * 4, base_channels * 2)\n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_channels * 2, base_channels)\n",
    "        self.out = nn.Conv2d(base_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.time_proj1 = nn.Linear(time_dim, base_channels)\n",
    "        self.time_proj2 = nn.Linear(time_dim, base_channels * 2)\n",
    "        self.time_proj3 = nn.Linear(time_dim, base_channels * 4)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        e1 = self.enc1(x) + self.time_proj1(t_emb)[:, :, None, None]\n",
    "        d1 = self.pool1(e1)\n",
    "        e2 = self.enc2(d1) + self.time_proj2(t_emb)[:, :, None, None]\n",
    "        d2 = self.pool2(e2)\n",
    "        e3 = self.enc3(d2) + self.time_proj3(t_emb)[:, :, None, None]\n",
    "        b = self.bottleneck(e3)\n",
    "        u1 = self.up1(b)\n",
    "        u1 = torch.cat([u1, e2], dim=1)\n",
    "        d1 = self.dec1(u1)\n",
    "        u2 = self.up2(d1)\n",
    "        u2 = torch.cat([u2, e1], dim=1)\n",
    "        d2 = self.dec2(u2)\n",
    "        out = self.out(d2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e594ba59-cd0c-484c-b57d-487fcf1d5e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, T, beta_start, beta_end):\n",
    "        self.T = T\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        # Calcola beta, alpha e alpha_bar\n",
    "        self.beta = torch.linspace(beta_start, beta_end, T)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise):\n",
    "        # Aggiunge rumore all'immagine x_0 al timestep t\n",
    "        self.alpha_bar = self.alpha_bar.to(t.device)  # Sposto self.alpha_bar sulla stessa device di t\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        return torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf574d9a-7c2a-4519-bf85-2763e525b15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a710c83-5946-442e-814c-04b2b16b4831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [0/938], Loss: 1.1071\n",
      "Epoch [1/30], Step [100/938], Loss: 0.1029\n",
      "Epoch [1/30], Step [200/938], Loss: 0.0606\n",
      "Epoch [1/30], Step [300/938], Loss: 0.0583\n",
      "Epoch [1/30], Step [400/938], Loss: 0.0356\n",
      "Epoch [1/30], Step [500/938], Loss: 0.0475\n",
      "Epoch [1/30], Step [600/938], Loss: 0.0568\n",
      "Epoch [1/30], Step [700/938], Loss: 0.0517\n",
      "Epoch [1/30], Step [800/938], Loss: 0.0388\n",
      "Epoch [1/30], Step [900/938], Loss: 0.0281\n",
      "Epoch [1/30] Average Loss: 0.0644\n",
      "Epoch [2/30], Step [0/938], Loss: 0.0341\n",
      "Epoch [2/30], Step [100/938], Loss: 0.0280\n",
      "Epoch [2/30], Step [200/938], Loss: 0.0311\n",
      "Epoch [2/30], Step [300/938], Loss: 0.0452\n",
      "Epoch [2/30], Step [400/938], Loss: 0.0315\n",
      "Epoch [2/30], Step [500/938], Loss: 0.0297\n",
      "Epoch [2/30], Step [600/938], Loss: 0.0308\n",
      "Epoch [2/30], Step [700/938], Loss: 0.0296\n",
      "Epoch [2/30], Step [800/938], Loss: 0.0273\n",
      "Epoch [2/30], Step [900/938], Loss: 0.0253\n",
      "Epoch [2/30] Average Loss: 0.0336\n",
      "Epoch [3/30], Step [0/938], Loss: 0.0266\n",
      "Epoch [3/30], Step [100/938], Loss: 0.0366\n",
      "Epoch [3/30], Step [200/938], Loss: 0.0323\n",
      "Epoch [3/30], Step [300/938], Loss: 0.0350\n",
      "Epoch [3/30], Step [400/938], Loss: 0.0289\n",
      "Epoch [3/30], Step [500/938], Loss: 0.0283\n",
      "Epoch [3/30], Step [600/938], Loss: 0.0364\n",
      "Epoch [3/30], Step [700/938], Loss: 0.0330\n",
      "Epoch [3/30], Step [800/938], Loss: 0.0351\n",
      "Epoch [3/30], Step [900/938], Loss: 0.0349\n",
      "Epoch [3/30] Average Loss: 0.0300\n",
      "Epoch [4/30], Step [0/938], Loss: 0.0272\n",
      "Epoch [4/30], Step [100/938], Loss: 0.0286\n",
      "Epoch [4/30], Step [200/938], Loss: 0.0315\n",
      "Epoch [4/30], Step [300/938], Loss: 0.0240\n",
      "Epoch [4/30], Step [400/938], Loss: 0.0259\n",
      "Epoch [4/30], Step [500/938], Loss: 0.0258\n",
      "Epoch [4/30], Step [600/938], Loss: 0.0345\n",
      "Epoch [4/30], Step [700/938], Loss: 0.0362\n",
      "Epoch [4/30], Step [800/938], Loss: 0.0229\n",
      "Epoch [4/30], Step [900/938], Loss: 0.0331\n",
      "Epoch [4/30] Average Loss: 0.0283\n",
      "Epoch [5/30], Step [0/938], Loss: 0.0293\n",
      "Epoch [5/30], Step [100/938], Loss: 0.0333\n",
      "Epoch [5/30], Step [200/938], Loss: 0.0225\n",
      "Epoch [5/30], Step [300/938], Loss: 0.0274\n",
      "Epoch [5/30], Step [400/938], Loss: 0.0276\n",
      "Epoch [5/30], Step [500/938], Loss: 0.0277\n",
      "Epoch [5/30], Step [600/938], Loss: 0.0273\n",
      "Epoch [5/30], Step [700/938], Loss: 0.0216\n",
      "Epoch [5/30], Step [800/938], Loss: 0.0314\n",
      "Epoch [5/30], Step [900/938], Loss: 0.0322\n",
      "Epoch [5/30] Average Loss: 0.0270\n",
      "Epoch [6/30], Step [0/938], Loss: 0.0243\n",
      "Epoch [6/30], Step [100/938], Loss: 0.0274\n",
      "Epoch [6/30], Step [200/938], Loss: 0.0264\n",
      "Epoch [6/30], Step [300/938], Loss: 0.0319\n",
      "Epoch [6/30], Step [400/938], Loss: 0.0329\n",
      "Epoch [6/30], Step [500/938], Loss: 0.0216\n",
      "Epoch [6/30], Step [600/938], Loss: 0.0209\n",
      "Epoch [6/30], Step [700/938], Loss: 0.0298\n",
      "Epoch [6/30], Step [800/938], Loss: 0.0295\n",
      "Epoch [6/30], Step [900/938], Loss: 0.0271\n",
      "Epoch [6/30] Average Loss: 0.0262\n",
      "Epoch [7/30], Step [0/938], Loss: 0.0278\n",
      "Epoch [7/30], Step [100/938], Loss: 0.0261\n",
      "Epoch [7/30], Step [200/938], Loss: 0.0258\n",
      "Epoch [7/30], Step [300/938], Loss: 0.0222\n",
      "Epoch [7/30], Step [400/938], Loss: 0.0257\n",
      "Epoch [7/30], Step [500/938], Loss: 0.0319\n",
      "Epoch [7/30], Step [600/938], Loss: 0.0244\n",
      "Epoch [7/30], Step [700/938], Loss: 0.0225\n",
      "Epoch [7/30], Step [800/938], Loss: 0.0276\n",
      "Epoch [7/30], Step [900/938], Loss: 0.0235\n",
      "Epoch [7/30] Average Loss: 0.0257\n",
      "Epoch [8/30], Step [0/938], Loss: 0.0216\n",
      "Epoch [8/30], Step [100/938], Loss: 0.0243\n",
      "Epoch [8/30], Step [200/938], Loss: 0.0197\n",
      "Epoch [8/30], Step [300/938], Loss: 0.0240\n",
      "Epoch [8/30], Step [400/938], Loss: 0.0200\n",
      "Epoch [8/30], Step [500/938], Loss: 0.0316\n",
      "Epoch [8/30], Step [600/938], Loss: 0.0153\n",
      "Epoch [8/30], Step [700/938], Loss: 0.0222\n",
      "Epoch [8/30], Step [800/938], Loss: 0.0295\n",
      "Epoch [8/30], Step [900/938], Loss: 0.0220\n",
      "Epoch [8/30] Average Loss: 0.0252\n",
      "Epoch [9/30], Step [0/938], Loss: 0.0246\n",
      "Epoch [9/30], Step [100/938], Loss: 0.0247\n",
      "Epoch [9/30], Step [200/938], Loss: 0.0269\n",
      "Epoch [9/30], Step [300/938], Loss: 0.0247\n",
      "Epoch [9/30], Step [400/938], Loss: 0.0237\n",
      "Epoch [9/30], Step [500/938], Loss: 0.0223\n",
      "Epoch [9/30], Step [600/938], Loss: 0.0266\n",
      "Epoch [9/30], Step [700/938], Loss: 0.0230\n",
      "Epoch [9/30], Step [800/938], Loss: 0.0207\n",
      "Epoch [9/30], Step [900/938], Loss: 0.0244\n",
      "Epoch [9/30] Average Loss: 0.0248\n",
      "Epoch [10/30], Step [0/938], Loss: 0.0216\n",
      "Epoch [10/30], Step [100/938], Loss: 0.0200\n",
      "Epoch [10/30], Step [200/938], Loss: 0.0224\n",
      "Epoch [10/30], Step [300/938], Loss: 0.0193\n",
      "Epoch [10/30], Step [400/938], Loss: 0.0216\n",
      "Epoch [10/30], Step [500/938], Loss: 0.0179\n",
      "Epoch [10/30], Step [600/938], Loss: 0.0320\n",
      "Epoch [10/30], Step [700/938], Loss: 0.0295\n",
      "Epoch [10/30], Step [800/938], Loss: 0.0307\n",
      "Epoch [10/30], Step [900/938], Loss: 0.0254\n",
      "Epoch [10/30] Average Loss: 0.0247\n",
      "Epoch [11/30], Step [0/938], Loss: 0.0244\n",
      "Epoch [11/30], Step [100/938], Loss: 0.0237\n",
      "Epoch [11/30], Step [200/938], Loss: 0.0212\n",
      "Epoch [11/30], Step [300/938], Loss: 0.0287\n",
      "Epoch [11/30], Step [400/938], Loss: 0.0233\n",
      "Epoch [11/30], Step [500/938], Loss: 0.0307\n",
      "Epoch [11/30], Step [600/938], Loss: 0.0214\n",
      "Epoch [11/30], Step [700/938], Loss: 0.0194\n",
      "Epoch [11/30], Step [800/938], Loss: 0.0248\n",
      "Epoch [11/30], Step [900/938], Loss: 0.0341\n",
      "Epoch [11/30] Average Loss: 0.0241\n",
      "Epoch [12/30], Step [0/938], Loss: 0.0184\n",
      "Epoch [12/30], Step [100/938], Loss: 0.0245\n",
      "Epoch [12/30], Step [200/938], Loss: 0.0248\n",
      "Epoch [12/30], Step [300/938], Loss: 0.0239\n",
      "Epoch [12/30], Step [400/938], Loss: 0.0183\n",
      "Epoch [12/30], Step [500/938], Loss: 0.0187\n",
      "Epoch [12/30], Step [600/938], Loss: 0.0166\n",
      "Epoch [12/30], Step [700/938], Loss: 0.0166\n",
      "Epoch [12/30], Step [800/938], Loss: 0.0212\n",
      "Epoch [12/30], Step [900/938], Loss: 0.0189\n",
      "Epoch [12/30] Average Loss: 0.0238\n",
      "Epoch [13/30], Step [0/938], Loss: 0.0203\n",
      "Epoch [13/30], Step [100/938], Loss: 0.0160\n",
      "Epoch [13/30], Step [200/938], Loss: 0.0229\n",
      "Epoch [13/30], Step [300/938], Loss: 0.0249\n",
      "Epoch [13/30], Step [400/938], Loss: 0.0217\n",
      "Epoch [13/30], Step [500/938], Loss: 0.0254\n",
      "Epoch [13/30], Step [600/938], Loss: 0.0207\n",
      "Epoch [13/30], Step [700/938], Loss: 0.0269\n",
      "Epoch [13/30], Step [800/938], Loss: 0.0193\n",
      "Epoch [13/30], Step [900/938], Loss: 0.0186\n",
      "Epoch [13/30] Average Loss: 0.0238\n",
      "Epoch [14/30], Step [0/938], Loss: 0.0214\n",
      "Epoch [14/30], Step [100/938], Loss: 0.0234\n",
      "Epoch [14/30], Step [200/938], Loss: 0.0421\n",
      "Epoch [14/30], Step [300/938], Loss: 0.0270\n",
      "Epoch [14/30], Step [400/938], Loss: 0.0209\n",
      "Epoch [14/30], Step [500/938], Loss: 0.0152\n",
      "Epoch [14/30], Step [600/938], Loss: 0.0191\n",
      "Epoch [14/30], Step [700/938], Loss: 0.0291\n",
      "Epoch [14/30], Step [800/938], Loss: 0.0191\n",
      "Epoch [14/30], Step [900/938], Loss: 0.0268\n",
      "Epoch [14/30] Average Loss: 0.0233\n",
      "Epoch [15/30], Step [0/938], Loss: 0.0361\n",
      "Epoch [15/30], Step [100/938], Loss: 0.0254\n",
      "Epoch [15/30], Step [200/938], Loss: 0.0191\n",
      "Epoch [15/30], Step [300/938], Loss: 0.0234\n",
      "Epoch [15/30], Step [400/938], Loss: 0.0251\n",
      "Epoch [15/30], Step [500/938], Loss: 0.0246\n",
      "Epoch [15/30], Step [600/938], Loss: 0.0226\n",
      "Epoch [15/30], Step [700/938], Loss: 0.0168\n",
      "Epoch [15/30], Step [800/938], Loss: 0.0351\n",
      "Epoch [15/30], Step [900/938], Loss: 0.0250\n",
      "Epoch [15/30] Average Loss: 0.0236\n",
      "Epoch [16/30], Step [0/938], Loss: 0.0275\n",
      "Epoch [16/30], Step [100/938], Loss: 0.0278\n",
      "Epoch [16/30], Step [200/938], Loss: 0.0230\n",
      "Epoch [16/30], Step [300/938], Loss: 0.0231\n",
      "Epoch [16/30], Step [400/938], Loss: 0.0229\n",
      "Epoch [16/30], Step [500/938], Loss: 0.0216\n",
      "Epoch [16/30], Step [600/938], Loss: 0.0251\n",
      "Epoch [16/30], Step [700/938], Loss: 0.0240\n",
      "Epoch [16/30], Step [800/938], Loss: 0.0328\n",
      "Epoch [16/30], Step [900/938], Loss: 0.0225\n",
      "Epoch [16/30] Average Loss: 0.0233\n",
      "Epoch [17/30], Step [0/938], Loss: 0.0230\n",
      "Epoch [17/30], Step [100/938], Loss: 0.0259\n",
      "Epoch [17/30], Step [200/938], Loss: 0.0232\n",
      "Epoch [17/30], Step [300/938], Loss: 0.0247\n",
      "Epoch [17/30], Step [400/938], Loss: 0.0253\n",
      "Epoch [17/30], Step [500/938], Loss: 0.0183\n",
      "Epoch [17/30], Step [600/938], Loss: 0.0230\n",
      "Epoch [17/30], Step [700/938], Loss: 0.0206\n",
      "Epoch [17/30], Step [800/938], Loss: 0.0174\n",
      "Epoch [17/30], Step [900/938], Loss: 0.0228\n",
      "Epoch [17/30] Average Loss: 0.0233\n",
      "Epoch [18/30], Step [0/938], Loss: 0.0193\n",
      "Epoch [18/30], Step [100/938], Loss: 0.0241\n",
      "Epoch [18/30], Step [200/938], Loss: 0.0205\n",
      "Epoch [18/30], Step [300/938], Loss: 0.0230\n",
      "Epoch [18/30], Step [400/938], Loss: 0.0184\n",
      "Epoch [18/30], Step [500/938], Loss: 0.0182\n",
      "Epoch [18/30], Step [600/938], Loss: 0.0246\n",
      "Epoch [18/30], Step [700/938], Loss: 0.0243\n",
      "Epoch [18/30], Step [800/938], Loss: 0.0197\n",
      "Epoch [18/30], Step [900/938], Loss: 0.0225\n",
      "Epoch [18/30] Average Loss: 0.0229\n",
      "Epoch [19/30], Step [0/938], Loss: 0.0329\n",
      "Epoch [19/30], Step [100/938], Loss: 0.0274\n",
      "Epoch [19/30], Step [200/938], Loss: 0.0273\n",
      "Epoch [19/30], Step [300/938], Loss: 0.0266\n",
      "Epoch [19/30], Step [400/938], Loss: 0.0204\n",
      "Epoch [19/30], Step [500/938], Loss: 0.0303\n",
      "Epoch [19/30], Step [600/938], Loss: 0.0225\n",
      "Epoch [19/30], Step [700/938], Loss: 0.0217\n",
      "Epoch [19/30], Step [800/938], Loss: 0.0329\n",
      "Epoch [19/30], Step [900/938], Loss: 0.0190\n",
      "Epoch [19/30] Average Loss: 0.0232\n",
      "Epoch [20/30], Step [0/938], Loss: 0.0236\n",
      "Epoch [20/30], Step [100/938], Loss: 0.0152\n",
      "Epoch [20/30], Step [200/938], Loss: 0.0217\n",
      "Epoch [20/30], Step [300/938], Loss: 0.0253\n",
      "Epoch [20/30], Step [400/938], Loss: 0.0217\n",
      "Epoch [20/30], Step [500/938], Loss: 0.0187\n",
      "Epoch [20/30], Step [600/938], Loss: 0.0254\n",
      "Epoch [20/30], Step [700/938], Loss: 0.0205\n",
      "Epoch [20/30], Step [800/938], Loss: 0.0222\n",
      "Epoch [20/30], Step [900/938], Loss: 0.0181\n",
      "Epoch [20/30] Average Loss: 0.0228\n",
      "Epoch [21/30], Step [0/938], Loss: 0.0188\n",
      "Epoch [21/30], Step [100/938], Loss: 0.0247\n",
      "Epoch [21/30], Step [200/938], Loss: 0.0256\n",
      "Epoch [21/30], Step [300/938], Loss: 0.0227\n",
      "Epoch [21/30], Step [400/938], Loss: 0.0232\n",
      "Epoch [21/30], Step [500/938], Loss: 0.0248\n",
      "Epoch [21/30], Step [600/938], Loss: 0.0229\n",
      "Epoch [21/30], Step [700/938], Loss: 0.0277\n",
      "Epoch [21/30], Step [800/938], Loss: 0.0243\n",
      "Epoch [21/30], Step [900/938], Loss: 0.0227\n",
      "Epoch [21/30] Average Loss: 0.0226\n",
      "Epoch [22/30], Step [0/938], Loss: 0.0229\n",
      "Epoch [22/30], Step [100/938], Loss: 0.0131\n",
      "Epoch [22/30], Step [200/938], Loss: 0.0350\n",
      "Epoch [22/30], Step [300/938], Loss: 0.0208\n",
      "Epoch [22/30], Step [400/938], Loss: 0.0186\n",
      "Epoch [22/30], Step [500/938], Loss: 0.0191\n",
      "Epoch [22/30], Step [600/938], Loss: 0.0151\n",
      "Epoch [22/30], Step [700/938], Loss: 0.0286\n",
      "Epoch [22/30], Step [800/938], Loss: 0.0250\n",
      "Epoch [22/30], Step [900/938], Loss: 0.0187\n",
      "Epoch [22/30] Average Loss: 0.0227\n",
      "Epoch [23/30], Step [0/938], Loss: 0.0263\n",
      "Epoch [23/30], Step [100/938], Loss: 0.0187\n",
      "Epoch [23/30], Step [200/938], Loss: 0.0214\n",
      "Epoch [23/30], Step [300/938], Loss: 0.0146\n",
      "Epoch [23/30], Step [400/938], Loss: 0.0196\n",
      "Epoch [23/30], Step [500/938], Loss: 0.0181\n",
      "Epoch [23/30], Step [600/938], Loss: 0.0339\n",
      "Epoch [23/30], Step [700/938], Loss: 0.0264\n",
      "Epoch [23/30], Step [800/938], Loss: 0.0282\n",
      "Epoch [23/30], Step [900/938], Loss: 0.0199\n",
      "Epoch [23/30] Average Loss: 0.0226\n",
      "Epoch [24/30], Step [0/938], Loss: 0.0223\n",
      "Epoch [24/30], Step [100/938], Loss: 0.0196\n",
      "Epoch [24/30], Step [200/938], Loss: 0.0252\n",
      "Epoch [24/30], Step [300/938], Loss: 0.0201\n",
      "Epoch [24/30], Step [400/938], Loss: 0.0210\n",
      "Epoch [24/30], Step [500/938], Loss: 0.0262\n",
      "Epoch [24/30], Step [600/938], Loss: 0.0215\n",
      "Epoch [24/30], Step [700/938], Loss: 0.0199\n",
      "Epoch [24/30], Step [800/938], Loss: 0.0259\n",
      "Epoch [24/30], Step [900/938], Loss: 0.0204\n",
      "Epoch [24/30] Average Loss: 0.0226\n",
      "Epoch [25/30], Step [0/938], Loss: 0.0243\n",
      "Epoch [25/30], Step [100/938], Loss: 0.0239\n",
      "Epoch [25/30], Step [200/938], Loss: 0.0276\n",
      "Epoch [25/30], Step [300/938], Loss: 0.0237\n",
      "Epoch [25/30], Step [400/938], Loss: 0.0217\n",
      "Epoch [25/30], Step [500/938], Loss: 0.0240\n",
      "Epoch [25/30], Step [600/938], Loss: 0.0310\n",
      "Epoch [25/30], Step [700/938], Loss: 0.0226\n",
      "Epoch [25/30], Step [800/938], Loss: 0.0244\n",
      "Epoch [25/30], Step [900/938], Loss: 0.0247\n",
      "Epoch [25/30] Average Loss: 0.0224\n",
      "Epoch [26/30], Step [0/938], Loss: 0.0271\n",
      "Epoch [26/30], Step [100/938], Loss: 0.0240\n",
      "Epoch [26/30], Step [200/938], Loss: 0.0268\n",
      "Epoch [26/30], Step [300/938], Loss: 0.0214\n",
      "Epoch [26/30], Step [400/938], Loss: 0.0313\n",
      "Epoch [26/30], Step [500/938], Loss: 0.0238\n",
      "Epoch [26/30], Step [600/938], Loss: 0.0208\n",
      "Epoch [26/30], Step [700/938], Loss: 0.0245\n",
      "Epoch [26/30], Step [800/938], Loss: 0.0183\n",
      "Epoch [26/30], Step [900/938], Loss: 0.0162\n",
      "Epoch [26/30] Average Loss: 0.0225\n",
      "Epoch [27/30], Step [0/938], Loss: 0.0226\n",
      "Epoch [27/30], Step [100/938], Loss: 0.0213\n",
      "Epoch [27/30], Step [200/938], Loss: 0.0203\n",
      "Epoch [27/30], Step [300/938], Loss: 0.0231\n",
      "Epoch [27/30], Step [400/938], Loss: 0.0174\n",
      "Epoch [27/30], Step [500/938], Loss: 0.0253\n",
      "Epoch [27/30], Step [600/938], Loss: 0.0218\n",
      "Epoch [27/30], Step [700/938], Loss: 0.0248\n",
      "Epoch [27/30], Step [800/938], Loss: 0.0204\n",
      "Epoch [27/30], Step [900/938], Loss: 0.0214\n",
      "Epoch [27/30] Average Loss: 0.0227\n",
      "Epoch [28/30], Step [0/938], Loss: 0.0213\n",
      "Epoch [28/30], Step [100/938], Loss: 0.0159\n",
      "Epoch [28/30], Step [200/938], Loss: 0.0227\n",
      "Epoch [28/30], Step [300/938], Loss: 0.0275\n",
      "Epoch [28/30], Step [400/938], Loss: 0.0267\n",
      "Epoch [28/30], Step [500/938], Loss: 0.0229\n",
      "Epoch [28/30], Step [600/938], Loss: 0.0270\n",
      "Epoch [28/30], Step [700/938], Loss: 0.0180\n",
      "Epoch [28/30], Step [800/938], Loss: 0.0180\n",
      "Epoch [28/30], Step [900/938], Loss: 0.0248\n",
      "Epoch [28/30] Average Loss: 0.0223\n",
      "Epoch [29/30], Step [0/938], Loss: 0.0189\n",
      "Epoch [29/30], Step [100/938], Loss: 0.0215\n",
      "Epoch [29/30], Step [200/938], Loss: 0.0255\n",
      "Epoch [29/30], Step [300/938], Loss: 0.0189\n",
      "Epoch [29/30], Step [400/938], Loss: 0.0200\n",
      "Epoch [29/30], Step [500/938], Loss: 0.0195\n",
      "Epoch [29/30], Step [600/938], Loss: 0.0205\n",
      "Epoch [29/30], Step [700/938], Loss: 0.0184\n",
      "Epoch [29/30], Step [800/938], Loss: 0.0275\n",
      "Epoch [29/30], Step [900/938], Loss: 0.0215\n",
      "Epoch [29/30] Average Loss: 0.0224\n",
      "Epoch [30/30], Step [0/938], Loss: 0.0192\n",
      "Epoch [30/30], Step [100/938], Loss: 0.0207\n",
      "Epoch [30/30], Step [200/938], Loss: 0.0190\n",
      "Epoch [30/30], Step [300/938], Loss: 0.0232\n",
      "Epoch [30/30], Step [400/938], Loss: 0.0219\n",
      "Epoch [30/30], Step [500/938], Loss: 0.0179\n",
      "Epoch [30/30], Step [600/938], Loss: 0.0298\n",
      "Epoch [30/30], Step [700/938], Loss: 0.0239\n",
      "Epoch [30/30], Step [800/938], Loss: 0.0192\n",
      "Epoch [30/30], Step [900/938], Loss: 0.0276\n",
      "Epoch [30/30] Average Loss: 0.0222\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # determino il dispositivo su cui eseguire il training (GPU integrata apple silicon)\n",
    "\n",
    "# Inizializzo i modelli\n",
    "scheduler = NoiseScheduler(T, beta_start, beta_end)\n",
    "unet = UNet().to(device) \n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate) # ottimizzatore per aggiornare i pesi della U-Net durante il training\n",
    "criterion = nn.MSELoss() # mi permette di misurare \n",
    "\n",
    "# Lista per salvare la loss media di ogni epoca\n",
    "losses = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    total_loss = 0\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0] # [64,1,28,28] --> 64 (batch_size)\n",
    "        \n",
    "        # Campiona timestep casuali\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "        \n",
    "        # Genera rumore e immagine rumorosa\n",
    "        noise = torch.randn_like(images)\n",
    "        noisy_images = scheduler.add_noise(images, t, noise)\n",
    "        \n",
    "        # Previsione del rumore\n",
    "        predicted_noise = unet(noisy_images, t)\n",
    "        \n",
    "        # Calcola la loss\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "        total_loss += loss.item() # accumulo la loss di ogni batch\n",
    "        \n",
    "        # Ottimizzazione\n",
    "        optimizer.zero_grad() # azzero i gradienti accumulati nei batch precedenti\n",
    "        loss.backward()\n",
    "        optimizer.step() # aggiorno i pesi della U-net \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calcolo e salvo la loss media\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Salvo il modello\n",
    "    torch.save(unet.state_dict(), os.path.join(checkpoint_dir, f'unet_epoch_{epoch+1}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30c2adc3-d0d7-43dc-a524-e1117f9fcb52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet.eval()\n",
    "\n",
    "with torch.no_grad(): # riduco l'utilizzo di memoria\n",
    "    # Campiono un'immagine di rumore casuale\n",
    "    x_t = torch.randn(1, 1, 28, 28).to(device)\n",
    "    \n",
    "    # Eseguo il processo inverso per generare l'immagine\n",
    "    for i in range(T-1, -1, -1):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device) # creo un tensore casuale\n",
    "        \n",
    "        # Predico il rumore\n",
    "        predicted_noise = unet(x_t, t)\n",
    "        \n",
    "        # Calcola i coefficienti di diffusione inversa\n",
    "        alpha_t = scheduler.alpha[i].to(device)\n",
    "        alpha_bar_t = scheduler.alpha_bar[i].to(device)\n",
    "        \n",
    "        if i > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            beta_t = scheduler.beta[i].to(device)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x_t)\n",
    "            beta_t = torch.tensor(0.0).to(device)\n",
    "            \n",
    "        # Formula di campionamento inverso DDPM\n",
    "        x_t = (1 / torch.sqrt(alpha_t)) * (x_t - (beta_t / torch.sqrt(1 - alpha_bar_t)) * predicted_noise) + torch.sqrt(beta_t) * noise\n",
    "    \n",
    "    # Denormalizza l'immagine (da [-0.5, 0.5] a [0, 1])\n",
    "    generated_image = (x_t.squeeze().cpu() * 0.5 + 0.5).numpy()\n",
    "    \n",
    "    # Mostro l'immagine generata\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(generated_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Immagine generata MNIST\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f452df-5878-47c5-9668-bee34c2db696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
